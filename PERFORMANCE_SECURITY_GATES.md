# Performance & Security Gates System

This document describes the comprehensive performance and security gates system implemented for the Arbiter project. The system provides automated benchmarking, security scanning, and quality gates that prevent performance regressions and security vulnerabilities from entering production.

## ğŸ¯ Overview

The performance and security gates system consists of:

1. **Performance Benchmarking Suite** - Comprehensive performance testing
2. **Security Scanning Integration** - Multi-layered security analysis
3. **Automated Quality Gates** - Pass/fail criteria for deployments
4. **Continuous Monitoring** - Historical tracking and trend analysis
5. **CI/CD Integration** - Automated gates in the development pipeline

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Performance       â”‚    â”‚    Security         â”‚    â”‚   Quality Gates     â”‚
â”‚   Benchmarking      â”‚â”€â”€â”€â”€â”‚    Scanning         â”‚â”€â”€â”€â”€â”‚   Evaluation        â”‚
â”‚                     â”‚    â”‚                     â”‚    â”‚                     â”‚
â”‚ â€¢ API Performance   â”‚    â”‚ â€¢ SAST Analysis     â”‚    â”‚ â€¢ Threshold Checks  â”‚
â”‚ â€¢ WebSocket Metrics â”‚    â”‚ â€¢ Dependency Scan   â”‚    â”‚ â€¢ Regression Detect â”‚
â”‚ â€¢ CUE Analysis      â”‚    â”‚ â€¢ Secrets Detection â”‚    â”‚ â€¢ Blocking Rules    â”‚
â”‚ â€¢ Memory Profiling  â”‚    â”‚ â€¢ Container Securityâ”‚    â”‚ â€¢ Trend Analysis    â”‚
â”‚ â€¢ Bundle Analysis   â”‚    â”‚ â€¢ API Security      â”‚    â”‚ â€¢ Report Generation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚    CI/CD Pipeline   â”‚
                            â”‚                     â”‚
                            â”‚ â€¢ GitHub Actions    â”‚
                            â”‚ â€¢ Quality Reports   â”‚
                            â”‚ â€¢ Deployment Gates  â”‚
                            â”‚ â€¢ Failure Blocking  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install Bun (if not already installed)
curl -fsSL https://bun.sh/install | bash

# Install CUE CLI
curl -sSL https://cuelang.org/go/install | sh

# Install Semgrep for security scanning
pip install semgrep
```

### Running Benchmarks

```bash
# Run full benchmark suite (performance + security)
bun run bench

# Run only performance benchmarks
bun run bench:performance  

# Run only security scans
bun run bench:security

# Quick benchmark (reduced test duration)
bun run bench:quick
```

### Understanding Results

Results are generated in multiple formats:

- **JSON Reports**: `benchmarks/reports/benchmark-results.json`
- **HTML Dashboard**: `benchmarks/reports/benchmark-report.html`  
- **Markdown Summary**: `benchmarks/reports/benchmark-report.md`
- **JUnit XML**: `benchmarks/reports/benchmark-junit.xml`

## ğŸ“Š Performance Benchmarking

### API Performance Testing

Tests API endpoints under various load conditions:

```typescript
// Configuration
const apiConfig = {
  baseUrl: "http://localhost:3001",
  concurrency: 10,        // Concurrent connections
  duration: 10,          // Test duration in seconds
  endpoints: ["/projects", "/analyze"],
};
```

**Metrics Tracked:**
- Response time (mean, p50, p75, p90, p95, p99)
- Throughput (requests per second)
- Error rates
- Concurrent request handling

**Quality Gates:**
- P95 response time < 500ms
- Throughput > 100 RPS
- Zero errors under normal load

### WebSocket Performance Testing

Tests real-time collaboration features:

```typescript
const wsConfig = {
  url: "ws://localhost:3001",
  connections: 50,           // Concurrent WebSocket connections
  messagesPerConnection: 100, // Messages per connection
  messageSize: 1024,         // Message size in bytes
};
```

**Metrics Tracked:**
- Connection establishment time
- Message latency (mean, p95)
- Message delivery rate
- Connection stability
- Throughput (messages/second, MB/s)

**Quality Gates:**
- P95 message latency < 100ms
- Message delivery rate â‰¥ 99%
- Connection success rate â‰¥ 90%

### CUE Analysis Performance

Tests the core CUE configuration analysis engine:

```typescript
const cueConfig = {
  sampleFiles: ["./examples/basic.cue", "./examples/complex.cue"],
  iterations: 100,
  timeout: 750,  // Must match production timeout
};
```

**Metrics Tracked:**
- Analysis execution time (mean, p95)
- Success rate
- Memory usage
- Evaluations per second

**Quality Gates:**
- Success rate â‰¥ 95%
- Average execution < 300ms
- P95 execution < 750ms (matches timeout)

### Memory Usage Analysis

Monitors memory consumption patterns:

```typescript
const memoryConfig = {
  iterations: 1000,
  gcForce: true,        // Force garbage collection
  leakThreshold: 10MB,  // Memory leak detection threshold
};
```

**Metrics Tracked:**
- Heap usage (baseline, peak, average)
- Memory growth over time
- Garbage collection efficiency
- Potential memory leaks

**Quality Gates:**
- Total memory growth < 50MB
- Zero potential memory leaks
- Memory efficiency score â‰¥ 75%

### Bundle Size Analysis

Tracks build output and bundle sizes:

**Metrics Tracked:**
- Total bundle size (web, api, shared packages)
- Build performance
- Chunk count and optimization
- Compression ratios

**Quality Gates:**
- Total bundle < 2MB
- Web bundle < 1.5MB
- Build time < 30 seconds
- Bundle growth < 25% vs baseline

## ğŸ”’ Security Scanning

### Static Application Security Testing (SAST)

Uses Semgrep for comprehensive code analysis:

```bash
# Rulesets applied
- p/security-audit     # General security patterns
- p/javascript        # JavaScript-specific issues  
- p/typescript        # TypeScript-specific issues
- p/docker           # Container security
- p/secrets          # Hardcoded secrets
- p/owasp-top-ten    # OWASP Top 10 vulnerabilities
```

**Detects:**
- Injection vulnerabilities (SQL, Command, XSS)
- Authentication bypasses
- Insecure configurations
- Cryptographic weaknesses
- Path traversal issues

### Dependency Vulnerability Scanning

Scans for known vulnerabilities in dependencies:

```bash
# Scanning methods
- bun audit              # Bun native audit
- npm audit (fallback)   # NPM audit for compatibility
- Package.json analysis  # Known vulnerable patterns
```

**Tracks:**
- CVE vulnerabilities in dependencies
- Outdated packages with known issues
- License compatibility issues
- Supply chain risks

### Secrets Detection

Identifies hardcoded secrets in source code:

**Detection Patterns:**
- API keys and tokens
- Database connection strings  
- Private keys and certificates
- Cloud credentials (AWS, etc.)
- Generic passwords and secrets

**Advanced Features:**
- Context analysis (excludes examples/placeholders)
- File type filtering
- False positive reduction
- Line-number reporting

### Container Security Scanning

Analyzes Docker containers for vulnerabilities:

```bash
# Scanning targets
- Base image vulnerabilities
- Configuration issues
- Exposed ports and services
- File permissions
```

### API Security Analysis

Tests API endpoints for security issues:

**Checks:**
- Authentication bypass attempts
- Authorization flaws
- Input validation weaknesses
- Rate limiting effectiveness
- CORS configuration

## ğŸš§ Quality Gates System

### Gate Configuration

Quality gates are defined with specific criteria:

```json
{
  "name": "API Response Time P95",
  "metric": "p95_latency_ms", 
  "operator": "lt",
  "threshold": 500,
  "baseline_comparison": false
}
```

### Gate Types

1. **Absolute Thresholds**: Fixed limits (e.g., response time < 500ms)
2. **Baseline Comparisons**: Relative to historical performance
3. **Regression Detection**: Prevents performance degradation
4. **Security Blockers**: Zero tolerance for critical vulnerabilities

### Failure Actions

- **Non-blocking**: Warnings that don't fail the build
- **Blocking**: Hard failures that prevent deployment
- **Regression**: Performance degradation detection

## ğŸ”„ Continuous Benchmarking

### Baseline Management

```bash
# Create new baseline (after significant improvements)
bun run --cwd packages/benchmarks baseline

# Compare against baseline
bun run bench  # Automatically compares if baseline exists
```

### Historical Tracking

- Performance metrics stored over time
- Trend analysis and regression detection
- Automatic alerts for performance degradation
- Comparison reports between builds

### Automated Optimization

- Identifies performance bottlenecks
- Suggests optimization opportunities
- Tracks improvement effectiveness
- Prevents performance regressions

## ğŸƒâ€â™‚ï¸ CI/CD Integration

### GitHub Actions Workflow

The system integrates with GitHub Actions:

```yaml
# .github/workflows/performance-security-gates.yml
name: Performance & Security Gates

on:
  push: [main, develop]
  pull_request: [main]

jobs:
  performance-benchmarks:    # Run performance tests
  security-scans:           # Execute security scanning
  bundle-analysis:          # Analyze bundle sizes
  integration-tests:        # Full integration testing
  quality-gate-summary:     # Final gate evaluation
```

### PR Integration

- **Automated Comments**: Performance and security results in PRs
- **Status Checks**: Pass/fail indicators
- **Detailed Reports**: Links to full analysis
- **Blocking Rules**: Prevents merge on critical issues

### Deployment Gates

- **Stage Gates**: Different criteria for staging vs production
- **Rollback Triggers**: Automatic rollback on performance issues
- **Monitoring Integration**: Real-time performance tracking

## ğŸ“ˆ Reporting & Monitoring

### Report Formats

1. **JSON**: Machine-readable results for automation
2. **HTML**: Interactive dashboard with charts
3. **Markdown**: Human-readable summaries  
4. **SARIF**: Security results in standard format
5. **JUnit XML**: CI/CD integration format

### Dashboard Features

- **Performance Trends**: Historical charts and graphs
- **Security Overview**: Vulnerability summaries
- **Gate Status**: Pass/fail indicators
- **Comparison Views**: Before/after analysis

### Alerting

- **Slack/Teams**: Real-time notifications
- **Email Reports**: Daily/weekly summaries
- **Webhook Integration**: Custom alert systems

## âš™ï¸ Configuration

### Performance Configuration

```json
{
  "performance": {
    "api": {
      "baseUrl": "http://localhost:3001",
      "concurrency": 10,
      "duration": 10
    },
    "websocket": {
      "connections": 50,
      "messagesPerConnection": 100
    }
  }
}
```

### Security Configuration

```json
{
  "security": {
    "scanners": {
      "sast": { "enabled": true, "timeout_ms": 300000 },
      "dependency": { "enabled": true },
      "secrets": { "enabled": true }
    },
    "gates": [
      {
        "name": "No Critical Vulnerabilities",
        "severity": "critical",
        "max_count": 0,
        "blocking": true
      }
    ]
  }
}
```

### Quality Gates Configuration

Quality gates can be customized per environment:

```json
{
  "gates": {
    "staging": {
      "performance_threshold": 1000,
      "security_blocking": false
    },
    "production": {
      "performance_threshold": 500,
      "security_blocking": true
    }
  }
}
```

## ğŸ› ï¸ Development Workflow

### Local Development

```bash
# 1. Start development servers
bun run dev

# 2. Run quick benchmarks during development
bun run bench:quick

# 3. Check specific aspects
bun run bench:performance  # Performance only
bun run bench:security    # Security only
```

### Pre-commit Hooks

```bash
# Install pre-commit hook
echo "bun run bench:quick" > .git/hooks/pre-push
chmod +x .git/hooks/pre-push
```

### IDE Integration

- **VSCode Extensions**: Real-time security scanning
- **Performance Profiling**: Built-in memory and CPU monitoring  
- **Quality Metrics**: Live feedback on code quality

## ğŸ”§ Troubleshooting

### Common Issues

**API Server Not Starting:**
```bash
# Check if port is in use
lsof -i :3001

# Kill existing processes
pkill -f "bun.*api"

# Start server manually
cd apps/api && bun run dev
```

**Performance Tests Failing:**
```bash
# Check API health
curl http://localhost:3001/health

# Verify CUE CLI installation
cue version

# Check system resources
free -h && top
```

**Security Scans Timing Out:**
```bash
# Install Semgrep
pip install semgrep

# Verify installation
semgrep --version

# Run with verbose output
semgrep --config=auto --verbose .
```

### Performance Tuning

**Optimize Test Duration:**
```json
{
  "api": { "duration": 5 },      // Reduce from 10s to 5s
  "websocket": { "connections": 25 }, // Reduce connections
  "cue": { "iterations": 50 }    // Reduce iterations
}
```

**Resource Limits:**
```bash
# Increase Node.js memory limit
export NODE_OPTIONS="--max-old-space-size=4096"

# Set test timeouts
export BENCHMARK_TIMEOUT=600000  # 10 minutes
```

## ğŸ“š Best Practices

### Performance Testing

1. **Consistent Environment**: Use same hardware/software for baselines
2. **Warm-up Periods**: Allow system to reach steady state
3. **Statistical Significance**: Run multiple iterations
4. **Realistic Load**: Test with production-like scenarios

### Security Scanning

1. **Regular Updates**: Keep security tools current
2. **False Positive Management**: Tune rules to reduce noise
3. **Comprehensive Coverage**: Scan all code, not just main branches
4. **Rapid Response**: Address critical issues immediately

### Quality Gates

1. **Gradual Tightening**: Start with loose gates, tighten over time
2. **Environment-Specific**: Different rules for dev/staging/prod
3. **Business Context**: Align gates with business requirements
4. **Regular Review**: Update gates as system evolves

## ğŸ”® Future Enhancements

### Planned Features

- **Machine Learning**: Predictive performance analysis
- **A/B Testing Integration**: Performance comparison frameworks
- **Real User Monitoring**: Production performance tracking
- **Advanced Security**: Runtime security monitoring
- **Chaos Engineering**: Failure injection testing

### Roadmap

- **Q1 2024**: Enhanced container security scanning
- **Q2 2024**: Machine learning-based anomaly detection  
- **Q3 2024**: Real-time performance monitoring
- **Q4 2024**: Advanced threat modeling integration

---

This performance and security gates system provides comprehensive quality assurance for the Arbiter project, ensuring that performance regressions and security vulnerabilities are caught early in the development process.